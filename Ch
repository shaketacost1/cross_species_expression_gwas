1) Export GWAS SNPs to BED (0-based)

We’ll read each Parquet, create a BED with chr, start=pos-1, end=pos, plus handy columns to keep (e.g., rsID, trait, p-value if present).

Run this from the repo root:



Rscript - <<'RSCRIPT'
suppressPackageStartupMessages({
  library(arrow); library(dplyr); library(readr); library(stringr)
})

dir.create("data/gwas/bed", showWarnings = FALSE, recursive = TRUE)

make_bed <- function(parquet_path, out_bed){
  df <- read_parquet(parquet_path) %>% as.data.frame()

  # Try to find columns robustly
  cols <- tolower(names(df))
  pick <- function(cands){
    i <- match(tolower(cands), cols)
    i <- i[!is.na(i)]
    if (length(i)) names(df)[i[1]] else NA_character_
  }

  chr_col <- pick(c("CHR_ID","chr_id","chr","chromosome","chrom"))
  pos_col <- pick(c("CHR_POS","chr_pos","position","pos","bp"))
  rs_col  <- pick(c("SNPS","snp","rsid","rs_id"))
  trait_col <- pick(c("DISEASE/TRAIT","disease_trait","trait"))
  p_col  <- pick(c("P-VALUE","p_value","pval","pvalue"))

  if (is.na(chr_col) || is.na(pos_col)) {
    stop("Could not find chromosome/position columns in ", parquet_path)
  }

  out <- df %>%
    transmute(
      chr = paste0("chr", !!as.name(chr_col)),
      start = as.numeric(!!as.name(pos_col)) - 1,
      end   = as.numeric(!!as.name(pos_col)),
      rsid  = if (!is.na(rs_col))  as.character(!!as.name(rs_col))  else NA_character_,
      trait = if (!is.na(trait_col)) as.character(!!as.name(trait_col)) else NA_character_,
      pval  = if (!is.na(p_col)) as.character(!!as.name(p_col)) else NA_character_
    ) %>%
    filter(!is.na(start), !is.na(end), !is.na(chr))

  # Basic cleanup: remove any rows with negative starts or non-standard chrom labels
  out <- out %>% filter(start >= 0, str_detect(chr, "^chr[0-9XYM]+$"))

  # Write BED (tab-delimited, no header)
  readr::write_tsv(out, out_bed, col_names = FALSE)
  message("Wrote: ", out_bed, " (", nrow(out), " rows)")
}

make_bed("data/gwas/clean/alcohol.parquet",       "data/gwas/bed/alcohol.snps.bed")
make_bed("data/gwas/clean/bmi.parquet",           "data/gwas/bed/bmi.snps.bed")
make_bed("data/gwas/clean/inflammation.parquet",  "data/gwas/bed/inflammation.snps.bed")
RSCRIPT

Quick sanity check:
head -3 data/gwas/bed/*.bed | sed -n '1,10p'

2) Sort the BEDs (required by bedtools)

for f in data/gwas/bed/*.snps.bed; do
  sort -k1,1 -k2,2n "$f" > "${f%.bed}.sorted.bed"
done



3) Intersect with CTCF peaks (GRCh38)

You already have the merged peaks at:
data/regulatory/human/derived/ctcf_merged.GRCh38.bed

Produce overlapped SNP lists (one per trait):

bedtools intersect -wa -u \
  -a data/gwas/bed/alcohol.snps.sorted.bed \
  -b data/regulatory/human/derived/ctcf_merged.GRCh38.bed \
  > data/regulatory/human/derived/alcohol_in_ctcf.bed

bedtools intersect -wa -u \
  -a data/gwas/bed/bmi.snps.sorted.bed \
  -b data/regulatory/human/derived/ctcf_merged.GRCh38.bed \
  > data/regulatory/human/derived/bmi_in_ctcf.bed

bedtools intersect -wa -u \
  -a data/gwas/bed/inflammation.snps.sorted.bed \
  -b data/regulatory/human/derived/ctcf_merged.GRCh38.bed \
  > data/regulatory/human/derived/inflammation_in_ctcf.bed



Explanation:
	•	-wa -u returns the SNP entries from -a that overlap at least one CTCF interval, without duplicating rows or appending the peak.

If you want to also keep the CTCF interval alongside each SNP, use -wa -wb and write to a separate file (it will have SNP columns followed by CTCF columns).




4) Summary counts

for t in alcohol bmi inflammation; do
  total=$(wc -l < data/gwas/bed/${t}.snps.sorted.bed | tr -d ' ')
  hit=$(wc -l < data/regulatory/human/derived/${t}_in_ctcf.bed | tr -d ' ')
  echo "${t}: ${hit}/${total} SNPs overlap CTCF"
done








mkdir -p data/regulatory/human/ctcf_raw
wget -c -i data/regulatory/human/ctcf_urls.txt -P data/regulatory/human/ctcf_raw



ls -lh data/regulatory/human/ctcf_raw | wc -l
du -sh data/regulatory/human/ctcf_raw
gzip -cd data/regulatory/human/ctcf_raw/*.bed.gz | head



mkdir -p data/regulatory/human/derived
# concat, keep standard chr*, sort, and merge
gzip -cd data/regulatory/human/ctcf_raw/*.bed.gz \
  | awk 'BEGIN{OFS="\t"} $1 ~ /^chr/ {print $1,$2,$3}' \
  | sort -k1,1 -k2,2n \
  | bedtools merge -i - \
  > data/regulatory/human/derived/ctcf_merged.GRCh38.bed




git lfs track "*.bed.gz"
git add .gitattributes \
       data/regulatory/human/ctcf_urls.txt \
       data/regulatory/human/ctcf_raw/*.bed.gz \
       data/regulatory/human/derived/ctcf_merged.GRCh38.bed
git commit -m "Add human CTCF (GRCh38 liver) narrowPeak files and merged set"
git push origin main


cat > data/regulatory/human/README.md <<'EOF'
CTCF (GRCh38, liver) narrowPeak files from ENCODE.
- Source: ENCODE Report with filters:
  type=File, assay_title=TF ChIP-seq, target=CTCF, status=released,
  file_type=bed narrowPeak, organ=liver, assembly=GRCh38
- Raw files: data/regulatory/human/ctcf_raw/*.bed.gz
- Merged set: data/regulatory/human/derived/ctcf_merged.GRCh38.bed (bedtools merge)
EOF
git add data/regulatory/human/README.md
git commit -m "Add README for human CTCF data (GRCh38 liver)"
git push origin main









F=data/regulatory/human/ctcf_report.tsv
jq -r '.["@graph"][]
  | select(.assembly=="GRCh38")
  | select(.file_type=="bed narrowPeak" or .file_type=="bigBed narrowPeak")
  | .href' "$F" \
| sed 's|^|https://www.encodeproject.org|g' \
> data/regulatory/human/ctcf_urls.txt


# Inspect result
head -5 data/regulatory/human/ctcf_urls.txt
wc -l data/regulatory/human/ctcf_urls.txt















# 1) Make sure folders exist
mkdir -p data/regulatory/human/ctcf_raw

# 2) Re-download the report explicitly as TSV
CTCF_TSV_URL="https://www.encodeproject.org/report/?type=File&assay_title=TF+ChIP-seq&target.label=CTCF&status=released&file_type=bed+narrowPeak&biosample_ontology.organ_slims=liver&assembly=GRCh38&limit=all&format=tsv"
curl -fsSL -H "Accept: text/tsv" "$CTCF_TSV_URL" -o data/regulatory/human/ctcf_report.tsv

# 3) Quick sanity check — first 2 lines should be a timestamp/URL line then a tabbed header row
sed -n '1,2p' data/regulatory/human/ctcf_report.tsv

# 4) Find the "Download URL" column index from the header (line 2)
COL=$(awk -F'\t' 'NR==2{for(i=1;i<=NF;i++) if($i=="Download URL"){print i; exit}}' data/regulatory/human/ctcf_report.tsv)
echo "Download URL col = $COL"

# 5) Extract the URLs (skip first 2 lines), prefix with domain when needed
tail -n +3 data/regulatory/human/ctcf_report.tsv \
| awk -F'\t' -v c="$COL" 'NF && $c!="" {u=$c; if(u ~ /^https?:\/\//) print u; else print "https://www.encodeproject.org" u}' \
> data/regulatory/human/ctcf_urls.txt

# 6) Inspect a few and count
head -5 data/regulatory/human/ctcf_urls.txt
wc -l data/regulatory/human/ctcf_urls.txt

# 7) Download the peaks
wget -c -i data/regulatory/human/ctcf_urls.txt -P data/regulatory/human/ctcf_raw

# 8) Verify content
ls -lh data/regulatory/human/ctcf_raw | wc -l
du -sh data/regulatory/human/ctcf_raw
gzip -cd data/regulatory/human/ctcf_raw/*.bed.gz | head














cd ~/my_project

# 1) Confirm the TSV exists and there are no weird chars in the path
ls -lb data/regulatory/human/ctcf_report.tsv
sed -n '1,3p' data/regulatory/human/ctcf_report.tsv | cat -A



# 2) Get the column index for "Download URL" (header is on line 2)
COL=$(awk -F'\t' 'NR==2{for(i=1;i<=NF;i++) if($i=="Download URL"){print i; exit}}' data/regulatory/human/ctcf_report.tsv)
echo "Download URL col = $COL"



tail -n +3 data/regulatory/human/ctcf_report.tsv \
| awk -F'\t' -v c="$COL" '{u=$c; if(u!=""){ if(u ~ /^https?:\/\//) print u; else print "https://www.encodeproject.org" u }}' \
> data/regulatory/human/ctcf_urls.txt

wc -l data/regulatory/human/ctcf_urls.txt
head -5 data/regulatory/human/ctcf_urls.txt

wget -c -i data/regulatory/human/ctcf_urls.txt -P data/regulatory/human/ctcf_raw

ls -lh data/regulatory/human/ctcf_raw | wc -l
gzip -cd data/regulatory/human/ctcf_raw/*.bed.gz | head




mkdir -p data/regulatory/human/{ctcf_raw,h3k27ac_raw}

CTCF_TSV_URL="https://www.encodeproject.org/report/?type=File&assay_title=TF+ChIP-seq&target.label=CTCF&status=released&file_type=bed+narrowPeak&biosample_ontology.organ_slims=liver&assembly=GRCh38&limit=all&format=tsv"
curl -fsSL -H "Accept: text/tsv" "$CTCF_TSV_URL" > data/regulatory/human/ctcf_report.tsv












cut -f20 file_report_2025_9_4_3h_31m.tsv | grep -v "Download URL" > ctcf_urls.txt


mkdir -p ctcf_raw
wget -c -i ctcf_urls.txt -P ctcf_raw

ls -lh ctcf_raw



COL=$(awk -F'\t' 'NR==1{for(i=1;i<=NF;i++) if($i=="Download URL"){print i; exit}}' ctcf_report.tsv)
echo "Download URL column index: $COL"


# extract URLs (skip header), drop blanks, make absolute URLs
awk -F'\t' -v c="$COL" 'NR>1 && $c!="" {print $c}' ctcf_report.tsv \
| awk '{u=$0; if(u ~ /^https?:\/\//) print u; else print "https://www.encodeproject.org" u}' \
> ctcf_urls.txt


# copy the report you downloaded into this folder first, or point to the full path
F=~/Downloads/file_report_2025_9_4_3h_31m.tsv

# find column indices
ACC=$(awk -F'\t' 'NR==2{for(i=1;i<=NF;i++) if($i=="Accession") print i}' "$F")
BIO=$(awk -F'\t' 'NR==2{for(i=1;i<=NF;i++) if($i=="Biosample name") print i}' "$F")
ORG=$(awk -F'\t' 'NR==2{for(i=1;i<=NF;i++) if($i=="Organ") print i}' "$F")
ASM=$(awk -F'\t' 'NR==2{for(i=1;i<=NF;i++) if($i=="Genome assembly") print i}' "$F")
OTY=$(awk -F'\t' 'NR==2{for(i=1;i<=NF;i++) if($i=="Output type") print i}' "$F")

# write metadata CSV (skip the first two header lines)
awk -F'\t' -v a="$ACC" -v b="$BIO" -v o="$ORG" -v g="$ASM" -v t="$OTY" 'NR>2{
  OFS=","; print $a,$b,$o,$g,$t
}' "$F" | (echo "accession,biosample,organ,assembly,output_type"; cat) > ctcf_metadata.csv


wc -l ctcf_urls.txt
head -5 ctcf_urls.txt


mkdir -p ctcf_raw
wget -c -i ctcf_urls.txt -P ctcf_raw


# how many files and total size?
ls -lh ctcf_raw | wc -l
du -sh ctcf_raw
# peek a file
gzip -cd ctcf_raw/*.bed.gz | head


# create a merged narrowPeak and tag each line with its source accession
echo -e "chrom\tstart\tend\tname\tscore\tstrand\tsignalValue\tpValue\tqValue\tsummit\tsource" > ctcf_merged.narrowPeak

for f in ctcf_raw/*.bed.gz; do
  acc=$(basename "$f" | sed -E 's/^([A-Z0-9]+).*/\1/')
  gzip -cd "$f" \
  | awk -v src="$acc" 'BEGIN{OFS="\t"} {print $1,$2,$3,$4,$5,$6,$7,$8,$9,$10,src}' \
  >> ctcf_merged.narrowPeak
done

# from the human/ directory where ctcf_raw/ lives
echo -e "chrom\tstart\tend\tname\tscore\tstrand\tsignalValue\tpValue\tqValue\tsummit\tsource" > ctcf_merged.narrowPeak

for f in ctcf_raw/*.bed.gz; do
  acc=$(basename "$f" | sed -E 's/^([A-Z0-9]+).*/\1/')
  gzip -cd "$f" \
  | awk -v src="$acc" 'BEGIN{OFS="\t"}{print $1,$2,$3,$4,$5,$6,$7,$8,$9,$10,src}' \
  >> ctcf_merged.narrowPeak
done

# sort for downstream compatibility (lexicographic chrom, numeric start)
LC_ALL=C sort -k1,1 -k2,2n ctcf_merged.narrowPeak -o ctcf_merged.narrowPeak

awk 'BEGIN{OFS="\t"} NR>1{print $1,$2,$3,$11}' ctcf_merged.narrowPeak > ctcf_merged.bed


# spot-check
head ctcf_merged.narrowPeak


mkdir -p ~/my_project/data/regulatory/human/ctcf
mv ctcf_raw ctcf_merged.narrowPeak ctcf_merged.bed ctcf_metadata.csv ~/my_project/data/regulatory/human/ctcf/


cd ~/my_project
git lfs track "*.bed.gz" "*.narrowPeak" "*.bed"
git add .gitattributes data/regulatory/human/ctcf
git commit -m "Add human CTCF narrowPeak peaks (GRCh38) + merged set and metadata"
git push origin main
